{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils import data\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv('/kaggle/input/g-research-crypto-forecasting/train.csv')\ndf.dropna(axis = 0, inplace = True)\n\n# auxiliary function, from datetime to timestamp\ntotimestamp = lambda s: np.int32(time.mktime(datetime.strptime(s, \"%d/%m/%Y\").timetuple()))","metadata":{"execution":{"iopub.status.busy":"2022-04-23T14:59:22.970424Z","iopub.execute_input":"2022-04-23T14:59:22.971247Z","iopub.status.idle":"2022-04-23T15:00:04.186575Z","shell.execute_reply.started":"2022-04-23T14:59:22.971212Z","shell.execute_reply":"2022-04-23T15:00:04.185645Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_window = [totimestamp(\"01/05/2021\"), totimestamp(\"30/05/2021\")]\ntest_window = [totimestamp(\"01/06/2021\"), totimestamp(\"30/06/2021\")]\nBATCH_SIZE = 5\nSEQ_LENGTH = 20\n\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)\n\ndef create_xy_pairs(X_series, y_series, seq_length):\n    data_length = len(X_series)\n    pairs = []\n    for idx in range(data_length - seq_length):\n        x = X_series[idx:idx + seq_length]\n        y = y_series[idx + seq_length:idx + seq_length + 1]\n        pairs.append((x, y))\n    return pairs","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:00:04.188721Z","iopub.execute_input":"2022-04-23T15:00:04.189045Z","iopub.status.idle":"2022-04-23T15:00:04.196821Z","shell.execute_reply.started":"2022-04-23T15:00:04.189004Z","shell.execute_reply":"2022-04-23T15:00:04.195925Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data = df.set_index(\"timestamp\")\nupper_shadow = lambda asset: asset.High - np.maximum(asset.Close,asset.Open)\nlower_shadow = lambda asset: np.minimum(asset.Close,asset.Open)- asset.Low\n\nX = pd.concat([log_return(data[data[\"Asset_ID\"]==0].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==0].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==0]), lower_shadow(data[data[\"Asset_ID\"]==0]), \\\n               log_return(data[data[\"Asset_ID\"]==1].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==1].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==1]), lower_shadow(data[data[\"Asset_ID\"]==1]), \\\n               log_return(data[data[\"Asset_ID\"]==2].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==2].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==2]), lower_shadow(data[data[\"Asset_ID\"]==2]), \\\n               log_return(data[data[\"Asset_ID\"]==3].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==3].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==3]), lower_shadow(data[data[\"Asset_ID\"]==3]), \\\n               log_return(data[data[\"Asset_ID\"]==4].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==4].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==4]), lower_shadow(data[data[\"Asset_ID\"]==4]), \\\n               log_return(data[data[\"Asset_ID\"]==5].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==5].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==5]), lower_shadow(data[data[\"Asset_ID\"]==5]), \\\n               log_return(data[data[\"Asset_ID\"]==6].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==6].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==6]), lower_shadow(data[data[\"Asset_ID\"]==6]), \\\n               log_return(data[data[\"Asset_ID\"]==7].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==7].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==7]), lower_shadow(data[data[\"Asset_ID\"]==7]), \\\n               log_return(data[data[\"Asset_ID\"]==8].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==8].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==8]), lower_shadow(data[data[\"Asset_ID\"]==8]), \\\n               log_return(data[data[\"Asset_ID\"]==9].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==9].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==9]), lower_shadow(data[data[\"Asset_ID\"]==9]), \\\n               log_return(data[data[\"Asset_ID\"]==10].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==10].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==10]), lower_shadow(data[data[\"Asset_ID\"]==10]), \\\n               log_return(data[data[\"Asset_ID\"]==11].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==11].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==11]), lower_shadow(data[data[\"Asset_ID\"]==11]), \\\n               log_return(data[data[\"Asset_ID\"]==12].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==12].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==12]), lower_shadow(data[data[\"Asset_ID\"]==12]), \\\n               log_return(data[data[\"Asset_ID\"]==13].VWAP,periods=5), \\\n               log_return(data[data[\"Asset_ID\"]==13].VWAP,periods=1).abs(), \\\n               upper_shadow(data[data[\"Asset_ID\"]==13]), lower_shadow(data[data[\"Asset_ID\"]==13]), \\\n              ], axis=1)\ny = data[data[\"Asset_ID\"]==0].Target\n\nX = X.loc[y.index].fillna(0)\n    \nX_train = X.loc[train_window[0]:train_window[1]].fillna(0).to_numpy()  # filling NaN's with zeros\ny_train = y.loc[train_window[0]:train_window[1]].fillna(0).to_numpy()  \n\nX_test = X.loc[test_window[0]:test_window[1]].fillna(0).to_numpy() \ny_test = y.loc[test_window[0]:test_window[1]].fillna(0).to_numpy() \n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n    \n    \ntrain = create_xy_pairs(X_train_scaled, y_train, SEQ_LENGTH)\ntest = create_xy_pairs(X_test_scaled, y_test, SEQ_LENGTH)\n\ntrain_dl = DataLoader(train, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)\ntest_dl = DataLoader(test, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-23T15:00:04.198456Z","iopub.execute_input":"2022-04-23T15:00:04.199065Z","iopub.status.idle":"2022-04-23T15:00:45.011330Z","shell.execute_reply.started":"2022-04-23T15:00:04.199023Z","shell.execute_reply":"2022-04-23T15:00:45.010389Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\nEPOCHS        = 100\nDROPOUT       = 0.1\nNUM_LAYERS    = 2\nINPUT_DIM     = 56\nOUTPUT_SIZE   = 1\nHIDDEN_SIZE   = 200\nLEARNING_RATE = 0.0001\nSTATE_DIM     = NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n        super(LSTM, self).__init__()\n\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def init_hidden(self, batch_size):\n        state_dim = (self.num_layers, batch_size, self.hidden_size)\n        return (torch.zeros(state_dim).to(device), torch.zeros(state_dim).to(device))\n\n    def forward(self, x, states):\n        x, (h, c) = self.lstm(x, states)\n        out = self.linear(x)\n        return out, (h, c)\n\nmodel = LSTM(\n    INPUT_DIM,\n    HIDDEN_SIZE,\n    NUM_LAYERS,\n    OUTPUT_SIZE,\n    DROPOUT\n).to(device)\n\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.linear.parameters(), lr=LEARNING_RATE, weight_decay=0.01)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:00:45.015895Z","iopub.execute_input":"2022-04-23T15:00:45.016254Z","iopub.status.idle":"2022-04-23T15:00:45.045235Z","shell.execute_reply.started":"2022-04-23T15:00:45.016212Z","shell.execute_reply":"2022-04-23T15:00:45.044346Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def training(model, epochs, validate_every=2):\n\n    training_losses = []\n    coefficient = []\n    \n    \n    # Initialize hidden and cell states with dimension:\n    # (num_layers * num_directions, batch, hidden_size)\n    states = model.init_hidden(BATCH_SIZE)\n\n    for epoch in tqdm(range(epochs)):\n\n        running_training_loss = 0.0\n        \n        model.train()\n        \n        # Begin training\n        for idx, (x_batch, y_batch) in enumerate(train_dl):\n            # Convert to Tensors\n            x_batch = x_batch.float().to(device)\n            y_batch = y_batch.float().to(device)\n      \n            # Truncated Backpropagation\n            states = [state.detach() for state in states]          \n\n            optimizer.zero_grad()\n\n            # Make prediction\n            output, states = model(x_batch, states)\n\n            # Calculate loss\n            loss = criterion(output[:, -1, :], y_batch)\n            loss.backward()\n            running_training_loss += loss.item()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n        \n        # Average loss across timesteps\n        training_losses.append(running_training_loss / len(train_dl))\n        \n        #predict\n        if (epoch+1) % 5 == 0:\n            # Set to eval mode\n            model.eval()\n            #torch.no_grad()\n            \n            pred = []\n            for idx, (x_batch, y_batch) in enumerate(test_dl):\n                # Convert to Tensors\n                x_batch = x_batch.float().to(device)\n                y_batch = y_batch.float().to(device)\n                \n                validation_states = [state.detach() for state in states]\n                output, _ = model(x_batch, validation_states)\n                output = output[:, -1, :].flatten().tolist()\n                pred += output\n            pred_btc = np.array(pred)\n            test_btc = y_test[20:len(pred_btc)+20]\n            coef = np.corrcoef(pred_btc, test_btc)[0,1]\n            coefficient.append(coef)\n            print('Epoch %i : corrcoef is %f.' %(epoch+1,coef))\n        \n        \n    # Visualize loss\n    epoch_count = range(1, len(training_losses) + 1)\n    plt.plot(epoch_count, training_losses, 'r--')\n    plt.legend(['Training Loss'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n\n    coef_epoch_count = range(5, (len(coefficient)+1)*5, 5)\n    plt.plot(coef_epoch_count, coefficient, 'b--')\n    plt.xticks(coef_epoch_count)\n    plt.legend(['Correlation Coefficient'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Coefficient')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:00:45.046776Z","iopub.execute_input":"2022-04-23T15:00:45.047330Z","iopub.status.idle":"2022-04-23T15:00:45.081019Z","shell.execute_reply.started":"2022-04-23T15:00:45.047288Z","shell.execute_reply":"2022-04-23T15:00:45.079868Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training(model, 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T15:00:45.083237Z","iopub.execute_input":"2022-04-23T15:00:45.083886Z","iopub.status.idle":"2022-04-23T16:15:01.925396Z","shell.execute_reply.started":"2022-04-23T15:00:45.083789Z","shell.execute_reply":"2022-04-23T16:15:01.924509Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\nEPOCHS        = 100\nDROPOUT       = 0.1\nNUM_LAYERS    = 3\nINPUT_DIM     = 56\nOUTPUT_SIZE   = 1\nHIDDEN_SIZE   = 100\nLEARNING_RATE = 0.0001\nSTATE_DIM     = NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n        super(LSTM, self).__init__()\n\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def init_hidden(self, batch_size):\n        state_dim = (self.num_layers, batch_size, self.hidden_size)\n        return (torch.zeros(state_dim).to(device), torch.zeros(state_dim).to(device))\n\n    def forward(self, x, states):\n        x, (h, c) = self.lstm(x, states)\n        out = self.linear(x)\n        return out, (h, c)\n\nmodel = LSTM(\n    INPUT_DIM,\n    HIDDEN_SIZE,\n    NUM_LAYERS,\n    OUTPUT_SIZE,\n    DROPOUT\n).to(device)\n\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.linear.parameters(), lr=LEARNING_RATE, weight_decay=0.01)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:15:01.927223Z","iopub.execute_input":"2022-04-23T16:15:01.927534Z","iopub.status.idle":"2022-04-23T16:15:01.947128Z","shell.execute_reply.started":"2022-04-23T16:15:01.927491Z","shell.execute_reply":"2022-04-23T16:15:01.946118Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"training(model, 40)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:15:01.948492Z","iopub.execute_input":"2022-04-23T16:15:01.949778Z","iopub.status.idle":"2022-04-23T16:49:42.502413Z","shell.execute_reply.started":"2022-04-23T16:15:01.949689Z","shell.execute_reply":"2022-04-23T16:49:42.501518Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\nEPOCHS        = 100\nDROPOUT       = 0.1\nNUM_LAYERS    = 4\nINPUT_DIM     = 56\nOUTPUT_SIZE   = 1\nHIDDEN_SIZE   = 56\nLEARNING_RATE = 0.0001\nSTATE_DIM     = NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n        super(LSTM, self).__init__()\n\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def init_hidden(self, batch_size):\n        state_dim = (self.num_layers, batch_size, self.hidden_size)\n        return (torch.zeros(state_dim).to(device), torch.zeros(state_dim).to(device))\n\n    def forward(self, x, states):\n        x, (h, c) = self.lstm(x, states)\n        out = self.linear(x)\n        return out, (h, c)\n\nmodel = LSTM(\n    INPUT_DIM,\n    HIDDEN_SIZE,\n    NUM_LAYERS,\n    OUTPUT_SIZE,\n    DROPOUT\n).to(device)\n\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.linear.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\ntraining(model, 50)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T16:49:42.504058Z","iopub.execute_input":"2022-04-23T16:49:42.504399Z","iopub.status.idle":"2022-04-23T17:38:36.656516Z","shell.execute_reply.started":"2022-04-23T16:49:42.504318Z","shell.execute_reply":"2022-04-23T17:38:36.655493Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\nEPOCHS        = 100\nDROPOUT       = 0.1\nNUM_LAYERS    = 10\nINPUT_DIM     = 56\nOUTPUT_SIZE   = 1\nHIDDEN_SIZE   = 56\nLEARNING_RATE = 0.0001\nSTATE_DIM     = NUM_LAYERS, BATCH_SIZE, HIDDEN_SIZE\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n        super(LSTM, self).__init__()\n\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def init_hidden(self, batch_size):\n        state_dim = (self.num_layers, batch_size, self.hidden_size)\n        return (torch.zeros(state_dim).to(device), torch.zeros(state_dim).to(device))\n\n    def forward(self, x, states):\n        x, (h, c) = self.lstm(x, states)\n        out = self.linear(x)\n        return out, (h, c)\n\nmodel = LSTM(\n    INPUT_DIM,\n    HIDDEN_SIZE,\n    NUM_LAYERS,\n    OUTPUT_SIZE,\n    DROPOUT\n).to(device)\n\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.linear.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\ntraining(model, 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T17:38:36.659724Z","iopub.execute_input":"2022-04-23T17:38:36.660218Z","iopub.status.idle":"2022-04-23T21:40:57.413455Z","shell.execute_reply.started":"2022-04-23T17:38:36.660174Z","shell.execute_reply":"2022-04-23T21:40:57.412484Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"training(model, 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T21:40:57.414735Z","iopub.execute_input":"2022-04-23T21:40:57.415040Z","iopub.status.idle":"2022-04-24T01:43:30.252629Z","shell.execute_reply.started":"2022-04-23T21:40:57.414984Z","shell.execute_reply":"2022-04-24T01:43:30.251533Z"},"trusted":true},"execution_count":18,"outputs":[]}]}